{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning and Pruning in Decision Trees\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as __searching the hyperparameter space for the optimum values__. In this lesson we'll look at some of the key hyper parameters for Decision Trees and how they affect the learning and prediction processes. \n",
    "\n",
    "## Objectives\n",
    "- Understand and explain the role of hyper parameter optimization in machine learning\n",
    "- IDentify the role of pruning while training a decision tree\n",
    "- Understand and explain key pruning parameters that can affect the predictive performance of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Optimization\n",
    "\n",
    ">__In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.__\n",
    "\n",
    "By contrast, the values of model parameters are derived via training as we have seen previously.\n",
    "Different model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, LASSO is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \n",
    "\n",
    "In this lesson we shall look at such optimizations in context of decision trees and see how these can effect the predictive performance as well the computational complexity of the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Pruning\n",
    "\n",
    "Now that we know how to grow a decision tree using python and scikit learn, let's move on and practice with the idea of __Optimization__ of the classifer. This means that we have some options available with decision trees that we can tweak before the actual learning takes place. \n",
    "\n",
    "A decision tree , grown beyond a certain level of complexity leads to over-fitting. If we grow our tree and carry on using poor predictors which dont have any impact on the accuracy will eventually a) slow down the learning,  and b) cause overfitting. \n",
    "\n",
    "> __This process of trimming decision trees to optimize the learning process is called \"Tree Pruning\".__\n",
    "\n",
    "We can prune our trees using the following parameters:\n",
    "\n",
    "- `Maximum Depth`\n",
    "\n",
    "Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\n",
    "\n",
    "- `Minimum Samples Leaf with Split`\n",
    "\n",
    "Restrict the size of sample leaf\n",
    "\n",
    "- `Minimum Leaf Sample Size `\n",
    "\n",
    "Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\n",
    "\n",
    "- `Maximum Leaf Nodes`\n",
    "\n",
    "Reduce the number of leaf nodes\n",
    "\n",
    "- `Maximum Features`\n",
    "\n",
    "Maximum number of features to consider when splitting a node\n",
    "\n",
    "Below we shall look at a selected hyper parameters and learn about their impact of the classifier performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `max_depth`\n",
    "\n",
    "The parameter for decision trees that we normally tune first is `max_depth`. This parameter indicates how deep we want our tree to be. IF the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result as __overfitting__ as we are learning granular information from given data , we make it difficult for our model to generalize for unseen data. This will result as a low training error but a large testing error.\n",
    "\n",
    "If, on the other hand, the tree is too shallow, we may run into __underfitting__, i.e. we are not learning enough information about the data and the accuracy of model stays low for both test and training samples . We fit a decision tree with depths ranging from 1 to 32 and plot the training and test auc scores.\n",
    "\n",
    "<img src=\"depth.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising , as the classifer learns more information from the data , but this can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth=4) in this case would be the first hyper parameter that we need to tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `min_samples_split`\n",
    "The hyper parameter `min_samples_split` is used to set the __minimum number of samples required to split an internal node__. This can vary between two extremes i.e. considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \n",
    "\n",
    "When we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\n",
    "\n",
    "<img src=\"split.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we see that the training and testing accuracy stabilize at certain min. sample split size , and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy that a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \n",
    "\n",
    "` max_dapth` and `min_samples+splt` are also both related to the computational cost involved with growing the tree. nLarge values for these parameters can create a complex, dense and long tree and with large datasets, it may become extremely time consuming to use default values.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `min_samples_leaf`\n",
    "\n",
    "This hyper parameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "\n",
    "<img src=\"leaf.png\" width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point results as a drop in accuracy. That is due to under-fitting again, as keeping too many samples in our leaf nodes mean that there is still a high level of uncertainty in the data. \n",
    "\n",
    "The main difference between the two is that `min_samples_leaf` guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though `min_samples_split` is more common in practice. These two hyper parameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\n",
    "\n",
    "For instance, if `min_samples_split = 5`, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If `min_samples_leaf = 2`, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less then the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there more such hyper-parameters ?\n",
    "\n",
    "In addition of above, Scikit Learn offers a number of other parameters for further fine tuning the learning process. [Consult the official doc](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to look at these parameters in detail. The parameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We shall shortly see this in action with a real dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources \n",
    "- [Overview of Hyper-parameter Tuning ](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview)\n",
    "- [Demystifying Hyper-parameter Tuning](https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f)\n",
    "- [Pruning Decision Trees](https://www.displayr.com/machine-learning-pruning-decision-trees/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson, we looked at the idea of hyper parameters optimization and how pruning plays an important role in restricting the growth of a decision tree , for our predictions to be accurate. We looked at a few hyper parameters which directly impact the potential over-fitting/underfitting in trees. Next we shall see these in prctice using scikit learn and python.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
