{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Pruning in Decision Trees\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as __searching the hyperparameter space for the optimum values__. In this lesson, we'll look at some of the key hyperparameters for decision trees and how they affect the learning and prediction processes. \n",
    "\n",
    "## Objectives \n",
    "\n",
    "- Identify the role of pruning while training decision trees  \n",
    "- List the different hyperparameters for tuning decision trees \n",
    "\n",
    "## Hyperparameter Optimization\n",
    "\n",
    ">__In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.__\n",
    "\n",
    "By contrast, the values of model parameters are derived via training as we have seen previously.\n",
    "Different model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, Lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \n",
    "\n",
    "In this lesson, we'll look at these sorts of optimizations in the context of decision trees and see how these can affect the predictive performance as well as the computational complexity of the tree. \n",
    "\n",
    "## Tree pruning\n",
    "\n",
    "Now that we know how to grow a decision tree using Python and scikit-learn, let's move on and practice __optimizing__ a classifier. We can tweak a few parameters in the decision tree algorithm before the actual learning takes place. \n",
    "\n",
    "A decision tree, grown beyond a certain level of complexity leads to overfitting. If we grow our tree and carry on using poor predictors that don't have any impact on the accuracy, we will eventually a) slow down the learning, and b) cause overfitting.  Different tree pruning parameters can adjust the amount of overfitting or underfitting in order to optimize for increased accuracy, precision, and/or recall.\n",
    "\n",
    "> __This process of trimming decision trees to optimize the learning process is called \"tree pruning\".__\n",
    "\n",
    "We can prune our trees using:\n",
    "\n",
    "- Maximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\n",
    "\n",
    "- Minimum samples leaf with split: Restrict the size of sample leaf\n",
    "\n",
    "- Minimum leaf sample size: Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\n",
    "\n",
    "- Maximum leaf nodes: Reduce the number of leaf nodes\n",
    "\n",
    "- Maximum features: Maximum number of features to consider when splitting a node\n",
    "\n",
    "Let's look at a few hyperparameters and learn about their impact on classifier performance:  \n",
    "\n",
    "\n",
    "## `max_depth`\n",
    "\n",
    "The parameter for decision trees that we normally tune first is `max_depth`. This parameter indicates how deep we want our tree to be. If the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result in __overfitting__ as it will lead to learning granular information from given data, which makes it difficult for our model to generalize on unseen data. \n",
    "Generally speaking, a low training error but a large testing error is a strong indication of this. \n",
    "\n",
    "If, on the other hand, the tree is too shallow, we may run into __underfitting__, i.e., we are not learning enough information about the data and the accuracy of the model stays low for both the test and training samples. The following example shows the training and test AUC scores for a decision tree with depths ranging from 1 to 32.\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/depth.png\" width=\"400\">\n",
    "\n",
    "\n",
    "In the above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising, as the classifier learns more information from the data. However this information can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth = 4) in this case would be the first hyperparameter that we need to tune. \n",
    "\n",
    "## `min_samples_split`\n",
    "\n",
    "The hyperparameter `min_samples_split` is used to set the __minimum number of samples required to split an internal node__. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \n",
    "\n",
    "When we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/split.png\" width=500>\n",
    "\n",
    "\n",
    "In the above plot, we see that the training and test accuracy stabilize at a certain minimum sample split size, and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy than a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \n",
    "\n",
    "> **Note**: `max_depth` and `min_samples_split` are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.  \n",
    "\n",
    "\n",
    "\n",
    "## `min_samples_leaf`\n",
    "\n",
    "This hyperparameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to `min_samples_splits`, however, this describes the minimum number of samples at the leaves, the base of the tree.\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/leaf.png\" width=400>\n",
    "\n",
    "\n",
    "\n",
    "The above plot shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point reduces accuracy. That is due to underfitting again, as keeping too many samples in our leaf nodes means that there is still a high level of uncertainty in the data. \n",
    "\n",
    "The main difference between the two is that `min_samples_leaf` guarantees a minimum number of samples in a leaf, while `min_samples_split` can create arbitrary small leaves, though `min_samples_split` is more common in practice. These two hyperparameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\n",
    "\n",
    "For instance, if `min_samples_split = 5`, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If `min_samples_leaf = 2`, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "### Are there more hyperparameters?\n",
    "\n",
    "Yes, there are! Scikit-learn offers a number of other hyperparameters for further fine-tuning the learning process. [Consult the official doc](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to look at them in detail. The hyperparameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We'll shortly see this in action with a real dataset. \n",
    "\n",
    "## Additional Resources \n",
    "- [Overview of hyperparameter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview)\n",
    "- [Demystifying hyperparameter tuning](https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f)\n",
    "- [Pruning decision trees](https://www.displayr.com/machine-learning-pruning-decision-trees/)\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lesson, we looked at the idea of optimizing hyperparameters and how pruning plays an important role in restricting the growth of a decision tree. We looked at a few hyperparameters which directly impact the potential overfitting/underfitting in trees. Next, we'll see these in practice using scikit-learn.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
